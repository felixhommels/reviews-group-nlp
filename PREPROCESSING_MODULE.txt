# --- Preprocessing Module Documentation ---

This document outlines the recent upgrade to the text preprocessing module. The previous NLTK-based pipeline has been replaced with a more powerful, multi-language system powered by spaCy.

## 1. Objective

The primary goal of this upgrade was to move from a single-language (English) preprocessing pipeline to a robust system capable of automatically detecting and processing reviews in multiple languages. This change significantly enhances the project's ability to handle diverse, real-world data.

## 2. Key Architectural Changes

The following changes were made to the project structure and dependencies:

*   **New Dependencies**: 
    *   `langdetect`: For automatic language detection of review text.
    *   `spacy`: For state-of-the-art tokenization, lemmatization, and stopword removal tailored to specific languages.
    *   These have been added to `requirements.txt`.

*   **New Core Preprocessor**:
    *   The old `src/preprocessing/preprocessor.py` has been **deleted**.
    *   It has been replaced by `src/preprocessing/spacy_preprocessor.py`, which contains the new pipeline logic.

*   **SpaCy Model Downloader**:
    *   A new script, `src/preprocessing/download_models.py`, has been created.
    *   **This is a mandatory, one-time script that must be run before using the pipeline.** It downloads the required language models from spaCy.

*   **Main Application Integration**:
    *   `app.py` has been updated to use the new `spacy_preprocessor`.
    *   The data flow within `app.py` now uses pandas DataFrames, which integrate seamlessly with the new pipeline.

## 3. How the New Pipeline Works

The new `preprocess_pipeline` function in `src/preprocessing/spacy_preprocessor.py` operates as follows:

1.  **Input**: It accepts a pandas DataFrame containing the review text.
2.  **Initial Cleaning**: It performs basic, language-agnostic cleaning (e.g., converting to lowercase, removing URLs and HTML tags).
3.  **Language Detection**: It iterates through each review, detects its language (e.g., 'en', 'es', 'fr'), and stores this information.
4.  **Language-Specific Processing**: Based on the detected language, it dynamically loads the corresponding spaCy model to perform advanced tokenization, lemmatization, and stopword removal. This ensures that each language is processed using the correct grammar and stopword lists.
5.  **Output**: It returns the DataFrame with new columns: `cleaned_text`, `language`, and `processed_text`.

## 4. Instructions for Setup and Use

To run the project with the new module, follow these steps:

1.  **Install Dependencies**: From the project root, run the following command to install `spacy`, `langdetect`, and other packages:
    ```
    pip install -r requirements.txt
    ```

2.  **Download Language Models**: This is a crucial one-time setup step. Run the new download script:
    ```
    python src/preprocessing/download_models.py
    ```

3.  **Run the Application**: You can now run the main application as usual. The new preprocessing logic is fully integrated.
    ```
    python app.py
    ```

## 5. Summary of New & Changed Files

*   **Kept/Modified**:
    *   `requirements.txt` (Updated)
    *   `app.py` (Updated)
    *   `src/preprocessing/spacy_preprocessor.py` (New)
    *   `src/preprocessing/download_models.py` (New)
    *   `src/__init__.py` & `src/utils/__init__.py` (New, to fix environment issues)

*   **Deleted**:
    *   `src/preprocessing/preprocessor.py` (Obsolete)

This new architecture provides a more scalable and accurate foundation for the subsequent stages of the NLP project. 